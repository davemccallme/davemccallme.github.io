{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\admin\\mambaforge\\lib\\site-packages (0.27.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\mambaforge\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from openai) (2.26.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\mambaforge\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from requests>=2.20->openai) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.2/1.5 MB 5.3 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 0.6/1.5 MB 4.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 3.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.1/1.5 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 1.2/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.4 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ---------------------------------------- 0.0/298.0 kB ? eta -:--:--\n",
      "     -------------------------------------  297.0/298.0 kB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 298.0/298.0 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2023.3.23-cp310-cp310-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 0.0/267.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/267.9 kB ? eta -:--:--\n",
      "     ------------------------------------  266.2/267.9 kB 16.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 267.9/267.9 kB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\mambaforge\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\mambaforge\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.2.0 nltk-3.8.1 regex-2023.3.23\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement re (from versions: none)\n",
      "ERROR: No matching distribution found for re\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\admin\\mambaforge\\lib\\site-packages (2.26.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 0.0/62.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 62.8/62.8 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from requests) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from requests) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\mambaforge\\lib\\site-packages (from requests) (1.26.14)\n",
      "Installing collected packages: requests\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "Successfully installed requests-2.28.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\mambaforge\\lib\\site-packages (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%%%Requirements.txt\n",
    "%pip install openai\n",
    "%pip install nltk\n",
    "%pip install re\n",
    "%pip install --upgrade requests\n",
    "%pip install --upgrade certifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.10.9\n",
      "re:  2.2.1\n",
      "nltk:  3.8.1\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import os\n",
    "import openai\n",
    "import ssl\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from os.path import splitext, exists\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "print('Python: ', platform.python_version())\n",
    "print('re: ', re.__version__)\n",
    "print('nltk: ', nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Documents\\GitHub\\davemccallme.github.io\\scratch\\LLM\\SONP-Discovery_2023-04-10-V_Davis.vtt \n",
      " C:\\Users\\admin\\Documents\\GitHub\\davemccallme.github.io\\scratch\\LLM\\SONP-Discovery_2023-04-10-V_Davis.txt\n"
     ]
    }
   ],
   "source": [
    "#update the file name to point to the desired transacription file \n",
    "#vttfile = r\"C:\\Users\\DIM6\\OneDrive - PGE\\Documents\\GitHub\\davemccallme.github.io\\scratch\\LLM\\SONP-Discovery_2023-04-10-V_Davis\"\n",
    "vttfile =   r\"C:\\Users\\admin\\Documents\\GitHub\\davemccallme.github.io\\scratch\\LLM\\SONP-Discovery_2023-04-10-V_Davis\"\n",
    "\n",
    "vtt = \".vtt\"\n",
    "txt = \".txt\"\n",
    "filepath = vttfile+vtt\n",
    "filename = vttfile+txt\n",
    "print(filepath, \"\\n\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up the content of a subtitle file\n",
    "def clean_webvtt(filepath: str) -> str:\n",
    "    \"\"\"Clean up the content of a subtitle file (vtt) to a string\n",
    "\n",
    "    Args:\n",
    "        filepath (str): path to vtt file\n",
    "\n",
    "    Returns:\n",
    "        str: clean content\n",
    "    \"\"\"\n",
    "    # read file content\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as fp:\n",
    "        content = fp.read()\n",
    "\n",
    "    # remove header & empty lines\n",
    "    lines = [line.strip() for line in content.split(\"\\n\") if line.strip()]\n",
    "    lines = lines[1:] if lines[0].upper() == \"WEBVTT\" else lines\n",
    "\n",
    "    # remove indexes\n",
    "    lines = [lines[i] for i in range(len(lines)) if not lines[i].isdigit()]\n",
    "\n",
    "    # remove tcode\n",
    "    #pattern = re.compile(r'^[0-9:.]{12} --> [0-9:.]{12}')\n",
    "    pattern = r'[a-f\\d]{8}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{12}\\/\\d+-\\d'\n",
    "    lines = [lines[i] for i in range(len(lines))\n",
    "             if not re.match(pattern, lines[i])]\n",
    "\n",
    "    # remove timestamps\n",
    "    pattern = r\"^\\d{2}:\\d{2}:\\d{2}.\\d{3}.*\\d{2}:\\d{2}:\\d{2}.\\d{3}$\"\n",
    "    lines = [lines[i] for i in range(len(lines))\n",
    "             if not re.match(pattern, lines[i])]\n",
    "\n",
    "    content = \" \".join(lines)\n",
    "\n",
    "    # remove duplicate spaces\n",
    "    pattern = r\"\\s+\"\n",
    "    content = re.sub(pattern, r\" \", content)\n",
    "\n",
    "    # add space after punctuation marks if it doesn't exist\n",
    "    pattern = r\"([\\.!?])(\\w)\"\n",
    "    content = re.sub(pattern, r\"\\1 \\2\", content)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the content of a subtitle file to text\n",
    "def vtt_to_clean_file(file_in: str, file_out=None, **kwargs) -> str:\n",
    "    \"\"\"Save clean content of a subtitle file to text file\n",
    "\n",
    "    Args:\n",
    "        file_in (str): path to vtt file\n",
    "        file_out (None, optional): path to text file\n",
    "        **kwargs (optional): arguments for other parameters\n",
    "            - no_message (bool): do not show message of result.\n",
    "                                 Default is False\n",
    "\n",
    "    Returns:\n",
    "        str: path to text file\n",
    "    \"\"\"\n",
    "    # set default values\n",
    "    no_message = kwargs.get(\"no_message\", False)\n",
    "    if not file_out:\n",
    "        filename = splitext(file_in)[0]\n",
    "        file_out = \"%s.txt\" % filename\n",
    "\n",
    "    content = clean_webvtt(file_in)\n",
    "    with open(file_out, \"w+\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(content)\n",
    "    if not no_message:\n",
    "        print(\"clean content is written to file: %s\" % file_out)\n",
    "\n",
    "    return file_out\n",
    "\n",
    "    vtt_to_clean_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count tokens for chunking API requests\n",
    "\n",
    "def count_tokens(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 13710\n"
     ]
    }
   ],
   "source": [
    "token_count = count_tokens(filename)\n",
    "print(f\"Number of tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the request chunks\n",
    "def break_up_file(tokens, chunk_size, overlap_size):\n",
    "    if len(tokens) <= chunk_size:\n",
    "        yield tokens\n",
    "    else:\n",
    "        chunk = tokens[:chunk_size]\n",
    "        yield chunk\n",
    "        yield from break_up_file(tokens[chunk_size-overlap_size:], chunk_size, overlap_size)\n",
    "\n",
    "def break_up_file_to_chunks(filename, chunk_size=2000, overlap_size=100):\n",
    "    with open(filename, 'r') as f:\n",
    "        text = f.read()\n",
    "    tokens = word_tokenize(text)\n",
    "    return list(break_up_file(tokens, chunk_size, overlap_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: 2000 tokens\n",
      "Chunk 1: 2000 tokens\n",
      "Chunk 2: 2000 tokens\n",
      "Chunk 3: 2000 tokens\n",
      "Chunk 4: 2000 tokens\n",
      "Chunk 5: 2000 tokens\n",
      "Chunk 6: 2000 tokens\n",
      "Chunk 7: 410 tokens\n"
     ]
    }
   ],
   "source": [
    "chunks = break_up_file_to_chunks(filename)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {len(chunk)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dim6 personal OpenAI API key - $$$ \n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'   \n",
    "openai.organization = \"org-AC2RGMuaXfFEbRfhmmkJ74Ef\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the responses from tokenized requests and correct spacing\n",
    "def convert_to_prompt_text(tokenized_text):\n",
    "    prompt_text = \" \".join(tokenized_text)\n",
    "    prompt_text = prompt_text.replace(\" 's\", \"'s\")\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uses OpenAI API requests to generate summaries of text data using language models with API parameters to fine-tune the summary generation process\n",
    "\n",
    "response1 = []\n",
    "\n",
    "chunks = break_up_file_to_chunks(filename)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    prompt_request = \"Summarize this meeting transcript: \" + convert_to_prompt_text(chunks[i])\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=prompt_request,\n",
    "            temperature=.5,\n",
    "            max_tokens=500,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    response1.append(response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI API conducts a conversation with a language model in order to refine a text summary generated by the model.\n",
    "#API request customized with parameters to fine-tune the summary generation and conversation processes.\n",
    "\n",
    "response2 = []\n",
    "\n",
    "chunks = break_up_file_to_chunks(filename)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    prompt_request = \"Summarize this meeting transcript: \" + convert_to_prompt_text(chunks[i])\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": \"This is text summarization.\"}]    \n",
    "    messages.append({\"role\": \"user\", \"content\": prompt_request})\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=messages,\n",
    "            temperature=.5,\n",
    "            max_tokens=500,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    response2.append(response[\"choices\"][0][\"message\"]['content'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidate the meeting summaries\n",
    "request = \"Consoloidate these meeting summaries: \" + str(prompt_response2)\n",
    "\n",
    "response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt_request,\n",
    "        temperature=.5,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vincent Davis discussed the current environment and encouraged everyone to maximize their ability within the current construct, take risks, and think outside the box. He also mentioned the importance of educating senior officers and the differences in how California reacted to the economic downturn due to COVID. He concluded the meeting by thanking everyone and offering his services if needed.\n"
     ]
    }
   ],
   "source": [
    "summary = response[\"choices\"][0][\"text\"].strip()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with the 3 stages of prompt_response\n",
    "data = {\"Stage 1\": response1,\n",
    "        \"Stage 2\": response2,\n",
    "        \"Stage 3\": request,\n",
    "        \"Stage 4\": summary}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export the DataFrame to Excel with 3 sheet names\n",
    "with pd.ExcelWriter('summary.xlsx') as writer:\n",
    "    df.to_excel(writer, sheet_name='Stages')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
